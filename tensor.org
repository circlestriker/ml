* python

** 调试
python3才安装了tensorflow，故
python3 test.py 可以正常执行
python test.py 报错找不到tensor模块  //python未指向python3


调试python:
To debug a script run the Emacs command “M-x pdb” and invoke Python’s pdb as “python -m pdb foo.py”

调试Jupyter notebooks
import pdb; pdb.set_trace()
在代码里插入上行， 再执行就可以调试了
from ipdb import set_trace 也可以

有语法高亮的办法
from IPython.core.debugger import set_trace
set_trace()


** 安装
IPython 6.0+ does not support Python 2.6, 2.7, 3.0, 3.1, or 3.2.
When using Python 2.7, please install IPython 5.x LTS Long Term Support version.
Beginning with IPython 6.0, Python 3.3 and above is required.

emacs+elpy+ipython

-mac安装pip
sudo easy_install pip

*** mac安装ipython
https://krishengreenwell.com/blog/how-i-installed-ipython-on-macos-10-13-1-high-sierra/

1.去这下载python3.6的安装包
https://www.python.org/downloads/

2.安装ipython
/usr/local/bin/pip3.6 install ipython

3. source .bash_profile

-安装python需要的包
pip install jedi flake8 autopep8 ipython

-安装elpy
需要>=24.4
然后按https://www.jianshu.com/p/4e48349f8ce6配置即可

查看代码是否符合风格
c-c c-v

-安装ipython
 pip install ipython 

在emacs里使用
(elpy-use-ipython)

-安装jupyter
sudo pip3 install jupyter

使用:jupyter notebook
然后选中对应的ipynb文件

sudo -H pip3 install pandas

*** ipython
ctrl+enter 执行


先下载
wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.5.0-cp36-cp36m-linux_x86_64.whl

* 安装
pip --version
需要8.1或更高版的 pip 才能顺利安装.

安装tensorflow
pip install tensorflow
参考:https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/1-2-install/

* 能做什么
手写体识别
验证码识别
语音识别
图像合成
** 智能推荐
+ 用户特征
常点的频道/地理位置/性别/年龄/讨厌的分类/职业
+ 资讯特征
分类/地理位置/热度/新鲜度

* 概念
同时使用输入数据和正确结果的训练方法叫做监督学习。还有一种叫做非监督学习，这种学习中只使用了输入数据而没有标签，但在这篇文章中我们不做讨论。

http://www.wolfib.com/Image-Recognition-Intro-Part-1/

kernel, filter, or feature detector: 卷积可以提取特征
CNN:多层卷积,再对结果执行非线性激活函数
图片分类:识别边-识别形状-识别高维特征-基于特征的分类器
Location Invariance(位置恒定性):不用关心大象在图中位置/能处理旋转缩放的情况
窄/宽卷积:Adding zero-padding is also called wide convolution, and not using zero-padding would be a narrow convolution.
Channels are different “views” of your input data.
适合用来分类:The most natural fit for CNNs seem to be classifications tasks, such as Sentiment Analysis, Spam Detection or Topic Categorization. 
** 激活函数
增加模型的非线性表达能力
激活函数的目的是利于分类:值越大，激活程度越高 对于分类，也就意味着它属于这一类的概率越大.各种激活函数层出不穷，各有优缺点. https://zhuanlan.zhihu.com/p/32824193
** Pooling Layers:
最常见的是取最大值。
用Pooling Layers的好处是可以得到固定size的输出矩阵。
会丢失位置信息
** 全连接层:
卷积层模仿人的视觉通路提取特征，全连接层一般负责分类或者回归，由于全连接层会丢失一些特征位置信息，所以最近FCN火了起来，全部卷积层，不用全连接层。
作用: 把分布式特征representation映射到样本标记空间。大大减少特征位置对分类带来的影响。
因为空间结构特性被忽略了，所以全连接层不适合用于在方位上找Pattern的任务
多层全连接层的意义: 比如要对子特征分类，也就是对猫头，猫尾巴，猫腿等进行分类
也是一次卷积。
** 词嵌入(即词向量)
如何让语言表示成为NN能够处理的数据类型。
能够帮助我们找到很难察觉的词语之间的关系。
统计语言模型正好具有捕捉上下文信息的能力.
词向量可以认为是神经网络训练语言模型的副产品. https://blog.csdn.net/u012052268/article/details/77170517
词向量既能够降低维度，又能够capture到当前词在本句子中上下文的信息（表现为前后距离关系）
1、选择使用别人训练好的词向量，注意，得使用相同语料内容领域的词向量；要么2、自己训练自己的词向量。我建议是前者，因为……坑太多了。

** 加权矩阵
** Biase
为何要有Biase?

* movie_recommender 
sklearn: scikit-learn: machine learning in Python
sudo -H pip3 install sklearn

scipy:科学计算和工程
sudo -H pip3 install scipy
numpy: 处理多维数据
pandas:数据分析包

预处理后的数据保存在了preprocess.p文件
pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))

* 笔记
如果我们一开始就抱着如何和代码交互的想法去研究Tensorflow，那就相当于在本质上走入歧途。

因为计算图只包含步骤，不包含结果！至少……现在还不包含！

一般来说，sess.run()是TensorFlow的最大瓶颈，你用的越少，程序就越好。只要有可能，我们应该让它一次性输出多个结果，而不是频繁使用，千万不要把它放进复杂循环。

* 朴素贝叶斯
https://blog.csdn.net/sinat_36246371/article/details/60140664
朴素: 特征独立; 每个特征的权重相同。

* 吴恩达机器学习

教授实现的西洋跳棋下棋程序能下赢教授自己。那么，我们实现的系统，是不是能够比我们更擅长为我们找合适的对象、实时地找问题的答案?

什么是线性回归
预测一个连续函数在某点的值

"回归"名字的由来:统计学者加尔顿高个父母的子女相对他们自己在变矮.

线性回归用最小二乘法求极小值。
能否用绝对值的和求最小值?
//可以, 绝对值也是一种距离或范数。只是大家普遍选择最小二乘法

svm可以支持无限多的特征

聚类只是无监督学习的一种

Octave可以快速实现相关算法

方阵才有逆矩阵, 是必要不充分条件

任何m*m矩阵都有逆矩阵?
不是。就像0没有倒数，元素全是0的方阵也无逆矩阵。
另外其他近似0方阵的矩阵也无逆矩阵，举例?

可以将无逆矩阵的矩阵理解为在某种方式上接近0矩阵

无逆矩阵的矩阵叫奇异(singular)矩阵或退化(degenerate)矩阵
** 梯度下降

可能收敛的不是很快。举例?

适用场景?

batch: 使用了全部数据集

正规方程组方法比梯度下降使用的步数更少
当n*n矩阵n是万级别时，求逆矩阵不方便，梯度下降优势开始显现。

? 大数据集上: 梯度下降比正规方程组方法更适用, 为什么

** 多元梯度下降

梯度下降的2个主要挑战: 
1)? 如何避免只找到局部最优解，未找到全局最优解
2)收敛(找到最优解的速度)速度太慢

线性回归的损伤函数是凸函数，无"只找到局部最优解"的问题:
Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum.

用房屋大小(size)x1、房间数(number of bedrooms)x2预测房价时, x1是[0,2000], x2是[1,5], 会导致过于狭长的等高线，收敛速度很慢。 
//解决:标准化:x1/2000, x2/5
feature scaling: mean normaliztion

可用pandas的分段函数cut()进行标准化

*** 预测股票的代价函数如何定义? 基于预测值-真实值?
**** 团队
**** 市场
**** 业绩
用户dau等数据

*** 学习率
吴恩达更倾向于看曲线图, 而非依赖自动收敛测试。
代码如何自动分析曲线图呢? 和自动收敛测试本质不是一样的吗?

learning rate alpha代表的是步长?
为什么选择更小的阿尔法(alpha)?
alpha也不能太小，否则可能收敛得很慢。

吴喜欢从[...,0.001,0.003,0.01,0.03,0.1,0.3,1,...]这样的范围找alpha。
3倍取点。这样找到一个太小的值，再找个太大的值。如何选择最大的可用值。

有时通过定义新的特性,比如用面积而不是长和宽, 可以得到更好的模型
** 正规方程
x'x不可逆的情况很少出现，而且即使出现, octave中, pinv(x'x)*x'*y可以处理x'x不可逆的情况
numpy也可处理这个情况

*** 矩阵不可逆的两种情况
1) X非列满秩?
行列式某两行or两列成比例，行列式为0，即矩阵不可逆。?
具体讲就是两个特征线性相关。如x1=size in feet^2, x2=size in m^2
2) 特征太多，以致样本数m<=特征数n
办法:删一些特征; 或正规化

? 相当于找最大线性无关组
? 线性变换
* 机器学习实战

一些属性具有长尾分布，因此你可能要将其进行转换（例如，计算其log对数）

用中位数填充训练集的缺失值时，也要用该中位数填充测试集中的缺失值，也可以用来实时替换新数据中的缺失值。

对复杂问题(如自然语言歧义消除)而言，数据比算法更重要。

p53, 为什么np.random.seed(42)可以让每次执行的数据是一样的?
** 第二章
*** 数据准备
id涉及到测试集的稳定性，即新实例加入时，新实例集的20%加入测试集，测试集原来的数据不变。//p53
用最稳定的特征创建id。经纬度是一个例子。

建立自己的转换函数库

中位数和平均数不同

drop函数默认删除行，列需要加axis = 1

估算器的关键词是fit:比如将缺失值用中位数估算填充。
转换器的关键词是transform: 转换器属于估算器的一种。能转换数据集的估算器。
预测器也是估算器的一种:能基于给定的数据集进行预测。

处理文本:
大部分机器学习算法更易于和数字打交道，需要将文本标签转为数字。
算法会以为2个相近数字更为相似一些。如果真实情况并非如此，可以用OneHotEncoder将整数分类值转为独热向量。
独热向量无相近数字，均是某一个元素是1其余为0的向量，避免了相似度误判。

标准化:平均值作为参照标准，故曰标准化。
*** 选择和训练模型
fit之后就可以得到模型了，就可以预测predict了

MSE: mean squared error
RMSE: root mean squared error

*** 交叉验证
产生背景: 决策树在整个训练集过拟合了，又不能动测试集，有个办法就是用部分训练集训练，剩余训练集验证。于是有了交叉验证。


housing.iloc[:5]

** 第四章

np.c_是concate两个矩阵

Octave有pinv计算伪逆矩阵, numpy也有numpy.linalg.pinv()

?? 如何从上山改为下山
Once you have the gradient vector, which points uphill, just go in the opposite direc‐ tion to go downhill.


? To find a good learning rate, you can use grid search(chp2)
|          |                  | 复杂度                               | 缺点                                                  |
| 梯度下降 | 逐步迭代         |                                      | 需要所有特征值的比例差不多,否则收敛慢(竖长的等高线图) |
| 正规方程 | 一次性直接出结果 | O(n^2.4)~O(n^3),n特征数;O(m),m样本数 | 特征数大(比如10万级)时计算缓慢                        |

O(m)如何得出的?

*** 批量梯度下降
? 批量是针对样本还是特征
//针对样本

*** 随机梯度下降(SGD)
批量梯度下降一般是稳步下降，SGD是一上一下来回跳地往下降。
SGD到了最优解附近会继续跳跃，导致最后得到的是近似最优解。

?? 为什么说:
There is almost no difference after training: all these algorithms end up with very similar models and make predictions in exactly the same way.

正规方程只能用在线性回归

学习曲线:x轴是训练样本数，y轴是rmse。训练的过程是学习过程，故叫学习曲线。

?何时用交叉验证
//需要估计模型的泛化能力时
If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it per‐ forms poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.

If your model is underfitting the training data, adding more train‐ ing examples will not help. You need to use a more complex model or come up with better features.
拟合不足要么模型不行，要么特征不行

One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error.
过拟合时，用更多训练数据直到验证误差接近训练误差
* d2l
** installation
conda activate d2l
Could not find conda environment: d2l
原因:没创建环境
解决:conda create --name d2l -y

(base) win:dev win$ conda activate d2l
(d2l) win:dev win$

更新conda
conda update -n base -c defaults conda

pip install torch==1.5.0 torchvision
ERROR: torchvision 0.6.1 has requirement torch==1.5.1, but you'll have torch 1.5.0 which is incompatible.
pip install torch==1.5.1 torchvision


Please always execute conda activate d2l to activate the runtime environment before running the code of the book or updating MXNet or the d2l package. To exit the environment, run conda deactivate.

** instroduction
Sequence Learning: 翻译、语音识别等。考虑过去和将来的样本
哪些问题不需要Sequence Learning? 
problems where we have some fixed number of inputs and produce a fixed number of outputs.
后面的输入和之前的输入没有关系的场景

为何叫深度学习:
Deep models are deep in precisely the sense that they learn many layers of computation.

distribution shift
穿越?

reinforcement learning and adversarial learning
*** 强化学习
actuator
The behavior of an RL agent is governed by a policy. In short, a policy is just a function that maps from observations (of the environment) to actions. The goal of reinforcement learning is to produce a good policy.

we can cast any supervised learning problem as an RL problem.
有时需要放弃短期利益:
potentially giving up some short-run reward in exchange for knowledge.

马尔科夫决策过程
When the environment is fully observed, we call the RL problem a Markov Decision Process (MDP). 
上下文赌博机。名字有点情况，和之前的action无关，为何还说是上下文?
When the state does not depend on the previous actions, we call the problem a contextual bandit problem. 
When there is no state, just a set of available actions with initially unknown rewards, this problem is the classic multi-armed bandit problem.


the autograd package automatically computes differentiation for us

** preliminaries
torch.randn(3, 4)
Each of its elements is randomly sampled from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1.

为什么有的元素大于1

a = torch.exp(torch.ones(2, 2))  # 得到2*2的全是e的Tensor
print(a)
print(torch.log(a))  # 取自然对数

>>> x
tensor([1., 2., 4., 8.])
>>> torch.exp(x)
tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])

e^1, e^2, e^4, e^8


broadcast: for matrix a it replicates the columns and for matrix b it replicates the rows before adding up both elementwise.
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
a加一列先变为
[0],[0]
[1],[1]
[2],[2]
b加2行变为
0,1
0,1
0,1
然后2个矩阵再相加

If the value of x is not reused in subsequent computations, we can also use x[:] = x + y or x += y to reduce the memory overhead of the operation.

>>> a
tensor([[[ 0.,  1.,  2.],
         [ 3.,  4.,  5.],
         [ 6.,  7.,  8.]],

        [[ 9., 10., 11.],
         [12., 13., 14.],
         [15., 16., 17.]],

        [[18., 19., 20.],
         [21., 22., 23.],
         [24., 25., 26.]]])
>>> b = torch.tensor([1,1,1,1]);
>>> b
tensor([1, 1, 1, 1])
>>> c =  a +b
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 2
>>>
>>> b = torch.tensor([[[[1]]]]); #[[[[4个括号
>>> b
tensor([[[[1]]]])
>>> c =  a +b    #??这里为什么可以加?
>>> c
tensor([[[[ 1.,  2.,  3.],
          [ 4.,  5.,  6.],
          [ 7.,  8.,  9.]],

         [[10., 11., 12.],
          [13., 14., 15.],
          [16., 17., 18.]],

         [[19., 20., 21.],
          [22., 23., 24.],
          [25., 26., 27.]]]])
>>> b2 = torch.tensor([[[1]]]);
>>> b2
tensor([[[1]]])
>>> a+b2
tensor([[[ 1.,  2.,  3.],
         [ 4.,  5.,  6.],
         [ 7.,  8.,  9.]],

        [[10., 11., 12.],
         [13., 14., 15.],
         [16., 17., 18.]],

        [[19., 20., 21.],
         [22., 23., 24.],
         [25., 26., 27.]]])
>>> a.size()
torch.Size([3, 3, 3])
>>> b.size()
torch.Size([1, 1, 1, 1])
>>> b2.size()
torch.Size([1, 1, 1])

* tf使用
AttributeError: module 'tensorflow' has no attribute 'Session'
tf.compat.v1.Session()

2.X用了eager_execution
* 数据挖掘算法竞赛
K折交叉验证: D1,D2,...,D10.
D1,...,D9去验证D10
除去D9的其他9个去验证D9
...
去除D1的其他9个去验证D1


** 数据穿越
*** 时间穿越
评估穿越，指的就是由于样本划分不当，导致测试集中的信息“穿越”到了训练集中，导致评估结果会更偏爱过拟合的模型，从而导致评估结果不够准确。
假如样本数据含7、8月份数据，8:2按随机方式选测试数据时，训练数据7、8月份的都有，验证数据也有7、8月份的, 即验证数据不能保证比训练数据新。
而模型使用时，往往处理的数据都是新数据。
这就导致模型的训练场景和实际预测时的场景不同。
参考https://blog.csdn.net/phrmgb/article/details/79997057

如果时间属性对数据无甚影响，这个时间穿越的问题就不大。

测试集和验证集的区别?


缺失值可能有业务含义 
