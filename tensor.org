* python

** è°ƒè¯•
python3æ‰å®‰è£…äº†tensorflowï¼Œæ•…
python3 test.py å¯ä»¥æ­£å¸¸æ‰§è¡Œ
python test.py æŠ¥é”™æ‰¾ä¸åˆ°tensoræ¨¡å—  //pythonæœªæŒ‡å‘python3


è°ƒè¯•python:
To debug a script run the Emacs command â€œM-x pdbâ€ and invoke Pythonâ€™s pdb as â€œpython -m pdb foo.pyâ€

è°ƒè¯•Jupyter notebooks
import pdb; pdb.set_trace()
åœ¨ä»£ç é‡Œæ’å…¥ä¸Šè¡Œï¼Œ å†æ‰§è¡Œå°±å¯ä»¥è°ƒè¯•äº†
from ipdb import set_trace ä¹Ÿå¯ä»¥

æœ‰è¯­æ³•é«˜äº®çš„åŠæ³•
from IPython.core.debugger import set_trace
set_trace()


** å®‰è£…
IPython 6.0+ does not support Python 2.6, 2.7, 3.0, 3.1, or 3.2.
When using Python 2.7, please install IPython 5.x LTS Long Term Support version.
Beginning with IPython 6.0, Python 3.3 and above is required.

emacs+elpy+ipython

-macå®‰è£…pip
sudo easy_install pip

*** macå®‰è£…ipython
https://krishengreenwell.com/blog/how-i-installed-ipython-on-macos-10-13-1-high-sierra/

1.å»è¿™ä¸‹è½½python3.6çš„å®‰è£…åŒ…
https://www.python.org/downloads/

2.å®‰è£…ipython
/usr/local/bin/pip3.6 install ipython

3. source .bash_profile

-å®‰è£…pythonéœ€è¦çš„åŒ…
pip install jedi flake8 autopep8 ipython

-å®‰è£…elpy
éœ€è¦>=24.4
ç„¶åæŒ‰https://www.jianshu.com/p/4e48349f8ce6é…ç½®å³å¯

æŸ¥çœ‹ä»£ç æ˜¯å¦ç¬¦åˆé£æ ¼
c-c c-v

-å®‰è£…ipython
 pip install ipython 

åœ¨emacsé‡Œä½¿ç”¨
(elpy-use-ipython)

-å®‰è£…jupyter
sudo pip3 install jupyter

ä½¿ç”¨:jupyter notebook
ç„¶åé€‰ä¸­å¯¹åº”çš„ipynbæ–‡ä»¶

sudo -H pip3 install pandas

*** ipython
ctrl+enter æ‰§è¡Œ


å…ˆä¸‹è½½
wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.5.0-cp36-cp36m-linux_x86_64.whl

* å®‰è£…
pip --version
éœ€è¦8.1æˆ–æ›´é«˜ç‰ˆçš„ pip æ‰èƒ½é¡ºåˆ©å®‰è£….

å®‰è£…tensorflow
pip install tensorflow
å‚è€ƒ:https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/1-2-install/

* èƒ½åšä»€ä¹ˆ
æ‰‹å†™ä½“è¯†åˆ«
éªŒè¯ç è¯†åˆ«
è¯­éŸ³è¯†åˆ«
å›¾åƒåˆæˆ
** æ™ºèƒ½æ¨è
+ ç”¨æˆ·ç‰¹å¾
å¸¸ç‚¹çš„é¢‘é“/åœ°ç†ä½ç½®/æ€§åˆ«/å¹´é¾„/è®¨åŒçš„åˆ†ç±»/èŒä¸š
+ èµ„è®¯ç‰¹å¾
åˆ†ç±»/åœ°ç†ä½ç½®/çƒ­åº¦/æ–°é²œåº¦

* æ¦‚å¿µ
åŒæ—¶ä½¿ç”¨è¾“å…¥æ•°æ®å’Œæ­£ç¡®ç»“æœçš„è®­ç»ƒæ–¹æ³•å«åšç›‘ç£å­¦ä¹ ã€‚è¿˜æœ‰ä¸€ç§å«åšéç›‘ç£å­¦ä¹ ï¼Œè¿™ç§å­¦ä¹ ä¸­åªä½¿ç”¨äº†è¾“å…¥æ•°æ®è€Œæ²¡æœ‰æ ‡ç­¾ï¼Œä½†åœ¨è¿™ç¯‡æ–‡ç« ä¸­æˆ‘ä»¬ä¸åšè®¨è®ºã€‚

http://www.wolfib.com/Image-Recognition-Intro-Part-1/

kernel, filter, or feature detector: å·ç§¯å¯ä»¥æå–ç‰¹å¾
CNN:å¤šå±‚å·ç§¯,å†å¯¹ç»“æœæ‰§è¡Œéçº¿æ€§æ¿€æ´»å‡½æ•°
å›¾ç‰‡åˆ†ç±»:è¯†åˆ«è¾¹-è¯†åˆ«å½¢çŠ¶-è¯†åˆ«é«˜ç»´ç‰¹å¾-åŸºäºç‰¹å¾çš„åˆ†ç±»å™¨
Location Invariance(ä½ç½®æ’å®šæ€§):ä¸ç”¨å…³å¿ƒå¤§è±¡åœ¨å›¾ä¸­ä½ç½®/èƒ½å¤„ç†æ—‹è½¬ç¼©æ”¾çš„æƒ…å†µ
çª„/å®½å·ç§¯:Adding zero-padding is also called wide convolution, and not using zero-padding would be a narrow convolution.
Channels are different â€œviewsâ€ of your input data.
é€‚åˆç”¨æ¥åˆ†ç±»:The most natural fit for CNNs seem to be classifications tasks, such as Sentiment Analysis, Spam Detection or Topic Categorization. 
** æ¿€æ´»å‡½æ•°
å¢åŠ æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
æ¿€æ´»å‡½æ•°çš„ç›®çš„æ˜¯åˆ©äºåˆ†ç±»:å€¼è¶Šå¤§ï¼Œæ¿€æ´»ç¨‹åº¦è¶Šé«˜ å¯¹äºåˆ†ç±»ï¼Œä¹Ÿå°±æ„å‘³ç€å®ƒå±äºè¿™ä¸€ç±»çš„æ¦‚ç‡è¶Šå¤§.å„ç§æ¿€æ´»å‡½æ•°å±‚å‡ºä¸ç©·ï¼Œå„æœ‰ä¼˜ç¼ºç‚¹. https://zhuanlan.zhihu.com/p/32824193
** Pooling Layers:
æœ€å¸¸è§çš„æ˜¯å–æœ€å¤§å€¼ã€‚
ç”¨Pooling Layersçš„å¥½å¤„æ˜¯å¯ä»¥å¾—åˆ°å›ºå®šsizeçš„è¾“å‡ºçŸ©é˜µã€‚
ä¼šä¸¢å¤±ä½ç½®ä¿¡æ¯
** å…¨è¿æ¥å±‚:
å·ç§¯å±‚æ¨¡ä»¿äººçš„è§†è§‰é€šè·¯æå–ç‰¹å¾ï¼Œå…¨è¿æ¥å±‚ä¸€èˆ¬è´Ÿè´£åˆ†ç±»æˆ–è€…å›å½’ï¼Œç”±äºå…¨è¿æ¥å±‚ä¼šä¸¢å¤±ä¸€äº›ç‰¹å¾ä½ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥æœ€è¿‘FCNç«äº†èµ·æ¥ï¼Œå…¨éƒ¨å·ç§¯å±‚ï¼Œä¸ç”¨å…¨è¿æ¥å±‚ã€‚
ä½œç”¨: æŠŠåˆ†å¸ƒå¼ç‰¹å¾representationæ˜ å°„åˆ°æ ·æœ¬æ ‡è®°ç©ºé—´ã€‚å¤§å¤§å‡å°‘ç‰¹å¾ä½ç½®å¯¹åˆ†ç±»å¸¦æ¥çš„å½±å“ã€‚
å› ä¸ºç©ºé—´ç»“æ„ç‰¹æ€§è¢«å¿½ç•¥äº†ï¼Œæ‰€ä»¥å…¨è¿æ¥å±‚ä¸é€‚åˆç”¨äºåœ¨æ–¹ä½ä¸Šæ‰¾Patternçš„ä»»åŠ¡
å¤šå±‚å…¨è¿æ¥å±‚çš„æ„ä¹‰: æ¯”å¦‚è¦å¯¹å­ç‰¹å¾åˆ†ç±»ï¼Œä¹Ÿå°±æ˜¯å¯¹çŒ«å¤´ï¼ŒçŒ«å°¾å·´ï¼ŒçŒ«è…¿ç­‰è¿›è¡Œåˆ†ç±»
ä¹Ÿæ˜¯ä¸€æ¬¡å·ç§¯ã€‚
** è¯åµŒå…¥(å³è¯å‘é‡)
å¦‚ä½•è®©è¯­è¨€è¡¨ç¤ºæˆä¸ºNNèƒ½å¤Ÿå¤„ç†çš„æ•°æ®ç±»å‹ã€‚
èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°å¾ˆéš¾å¯Ÿè§‰çš„è¯è¯­ä¹‹é—´çš„å…³ç³»ã€‚
ç»Ÿè®¡è¯­è¨€æ¨¡å‹æ­£å¥½å…·æœ‰æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›.
è¯å‘é‡å¯ä»¥è®¤ä¸ºæ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒè¯­è¨€æ¨¡å‹çš„å‰¯äº§å“. https://blog.csdn.net/u012052268/article/details/77170517
è¯å‘é‡æ—¢èƒ½å¤Ÿé™ä½ç»´åº¦ï¼Œåˆèƒ½å¤Ÿcaptureåˆ°å½“å‰è¯åœ¨æœ¬å¥å­ä¸­ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼ˆè¡¨ç°ä¸ºå‰åè·ç¦»å…³ç³»ï¼‰
1ã€é€‰æ‹©ä½¿ç”¨åˆ«äººè®­ç»ƒå¥½çš„è¯å‘é‡ï¼Œæ³¨æ„ï¼Œå¾—ä½¿ç”¨ç›¸åŒè¯­æ–™å†…å®¹é¢†åŸŸçš„è¯å‘é‡ï¼›è¦ä¹ˆ2ã€è‡ªå·±è®­ç»ƒè‡ªå·±çš„è¯å‘é‡ã€‚æˆ‘å»ºè®®æ˜¯å‰è€…ï¼Œå› ä¸ºâ€¦â€¦å‘å¤ªå¤šäº†ã€‚

** åŠ æƒçŸ©é˜µ
** Biase
ä¸ºä½•è¦æœ‰Biase?

* movie_recommender 
sklearn: scikit-learn: machine learning in Python
sudo -H pip3 install sklearn

scipy:ç§‘å­¦è®¡ç®—å’Œå·¥ç¨‹
sudo -H pip3 install scipy
numpy: å¤„ç†å¤šç»´æ•°æ®
pandas:æ•°æ®åˆ†æåŒ…

é¢„å¤„ç†åçš„æ•°æ®ä¿å­˜åœ¨äº†preprocess.pæ–‡ä»¶
pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))

* ç¬”è®°
å¦‚æœæˆ‘ä»¬ä¸€å¼€å§‹å°±æŠ±ç€å¦‚ä½•å’Œä»£ç äº¤äº’çš„æƒ³æ³•å»ç ”ç©¶Tensorflowï¼Œé‚£å°±ç›¸å½“äºåœ¨æœ¬è´¨ä¸Šèµ°å…¥æ­§é€”ã€‚

å› ä¸ºè®¡ç®—å›¾åªåŒ…å«æ­¥éª¤ï¼Œä¸åŒ…å«ç»“æœï¼è‡³å°‘â€¦â€¦ç°åœ¨è¿˜ä¸åŒ…å«ï¼

ä¸€èˆ¬æ¥è¯´ï¼Œsess.run()æ˜¯TensorFlowçš„æœ€å¤§ç“¶é¢ˆï¼Œä½ ç”¨çš„è¶Šå°‘ï¼Œç¨‹åºå°±è¶Šå¥½ã€‚åªè¦æœ‰å¯èƒ½ï¼Œæˆ‘ä»¬åº”è¯¥è®©å®ƒä¸€æ¬¡æ€§è¾“å‡ºå¤šä¸ªç»“æœï¼Œè€Œä¸æ˜¯é¢‘ç¹ä½¿ç”¨ï¼Œåƒä¸‡ä¸è¦æŠŠå®ƒæ”¾è¿›å¤æ‚å¾ªç¯ã€‚

* æœ´ç´ è´å¶æ–¯
https://blog.csdn.net/sinat_36246371/article/details/60140664
æœ´ç´ : ç‰¹å¾ç‹¬ç«‹; æ¯ä¸ªç‰¹å¾çš„æƒé‡ç›¸åŒã€‚

* å´æ©è¾¾æœºå™¨å­¦ä¹ 

æ•™æˆå®ç°çš„è¥¿æ´‹è·³æ£‹ä¸‹æ£‹ç¨‹åºèƒ½ä¸‹èµ¢æ•™æˆè‡ªå·±ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å®ç°çš„ç³»ç»Ÿï¼Œæ˜¯ä¸æ˜¯èƒ½å¤Ÿæ¯”æˆ‘ä»¬æ›´æ“…é•¿ä¸ºæˆ‘ä»¬æ‰¾åˆé€‚çš„å¯¹è±¡ã€å®æ—¶åœ°æ‰¾é—®é¢˜çš„ç­”æ¡ˆ?

ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’
é¢„æµ‹ä¸€ä¸ªè¿ç»­å‡½æ•°åœ¨æŸç‚¹çš„å€¼

"å›å½’"åå­—çš„ç”±æ¥:ç»Ÿè®¡å­¦è€…åŠ å°”é¡¿é«˜ä¸ªçˆ¶æ¯çš„å­å¥³ç›¸å¯¹ä»–ä»¬è‡ªå·±åœ¨å˜çŸ®.

çº¿æ€§å›å½’ç”¨æœ€å°äºŒä¹˜æ³•æ±‚æå°å€¼ã€‚
èƒ½å¦ç”¨ç»å¯¹å€¼çš„å’Œæ±‚æœ€å°å€¼?
//å¯ä»¥, ç»å¯¹å€¼ä¹Ÿæ˜¯ä¸€ç§è·ç¦»æˆ–èŒƒæ•°ã€‚åªæ˜¯å¤§å®¶æ™®éé€‰æ‹©æœ€å°äºŒä¹˜æ³•

svmå¯ä»¥æ”¯æŒæ— é™å¤šçš„ç‰¹å¾

èšç±»åªæ˜¯æ— ç›‘ç£å­¦ä¹ çš„ä¸€ç§

Octaveå¯ä»¥å¿«é€Ÿå®ç°ç›¸å…³ç®—æ³•

æ–¹é˜µæ‰æœ‰é€†çŸ©é˜µ, æ˜¯å¿…è¦ä¸å……åˆ†æ¡ä»¶

ä»»ä½•m*mçŸ©é˜µéƒ½æœ‰é€†çŸ©é˜µ?
ä¸æ˜¯ã€‚å°±åƒ0æ²¡æœ‰å€’æ•°ï¼Œå…ƒç´ å…¨æ˜¯0çš„æ–¹é˜µä¹Ÿæ— é€†çŸ©é˜µã€‚
å¦å¤–å…¶ä»–è¿‘ä¼¼0æ–¹é˜µçš„çŸ©é˜µä¹Ÿæ— é€†çŸ©é˜µï¼Œä¸¾ä¾‹?

å¯ä»¥å°†æ— é€†çŸ©é˜µçš„çŸ©é˜µç†è§£ä¸ºåœ¨æŸç§æ–¹å¼ä¸Šæ¥è¿‘0çŸ©é˜µ

æ— é€†çŸ©é˜µçš„çŸ©é˜µå«å¥‡å¼‚(singular)çŸ©é˜µæˆ–é€€åŒ–(degenerate)çŸ©é˜µ
** æ¢¯åº¦ä¸‹é™

å¯èƒ½æ”¶æ•›çš„ä¸æ˜¯å¾ˆå¿«ã€‚ä¸¾ä¾‹?

é€‚ç”¨åœºæ™¯?

batch: ä½¿ç”¨äº†å…¨éƒ¨æ•°æ®é›†

æ­£è§„æ–¹ç¨‹ç»„æ–¹æ³•æ¯”æ¢¯åº¦ä¸‹é™ä½¿ç”¨çš„æ­¥æ•°æ›´å°‘
å½“n*nçŸ©é˜µnæ˜¯ä¸‡çº§åˆ«æ—¶ï¼Œæ±‚é€†çŸ©é˜µä¸æ–¹ä¾¿ï¼Œæ¢¯åº¦ä¸‹é™ä¼˜åŠ¿å¼€å§‹æ˜¾ç°ã€‚

? å¤§æ•°æ®é›†ä¸Š: æ¢¯åº¦ä¸‹é™æ¯”æ­£è§„æ–¹ç¨‹ç»„æ–¹æ³•æ›´é€‚ç”¨, ä¸ºä»€ä¹ˆ

** å¤šå…ƒæ¢¯åº¦ä¸‹é™

æ¢¯åº¦ä¸‹é™çš„2ä¸ªä¸»è¦æŒ‘æˆ˜: 
1)? å¦‚ä½•é¿å…åªæ‰¾åˆ°å±€éƒ¨æœ€ä¼˜è§£ï¼Œæœªæ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£
2)æ”¶æ•›(æ‰¾åˆ°æœ€ä¼˜è§£çš„é€Ÿåº¦)é€Ÿåº¦å¤ªæ…¢

çº¿æ€§å›å½’çš„æŸä¼¤å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œæ— "åªæ‰¾åˆ°å±€éƒ¨æœ€ä¼˜è§£"çš„é—®é¢˜:
Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum.

ç”¨æˆ¿å±‹å¤§å°(size)x1ã€æˆ¿é—´æ•°(number of bedrooms)x2é¢„æµ‹æˆ¿ä»·æ—¶, x1æ˜¯[0,2000], x2æ˜¯[1,5], ä¼šå¯¼è‡´è¿‡äºç‹­é•¿çš„ç­‰é«˜çº¿ï¼Œæ”¶æ•›é€Ÿåº¦å¾ˆæ…¢ã€‚ 
//è§£å†³:æ ‡å‡†åŒ–:x1/2000, x2/5
feature scaling: mean normaliztion

å¯ç”¨pandasçš„åˆ†æ®µå‡½æ•°cut()è¿›è¡Œæ ‡å‡†åŒ–

*** é¢„æµ‹è‚¡ç¥¨çš„ä»£ä»·å‡½æ•°å¦‚ä½•å®šä¹‰? åŸºäºé¢„æµ‹å€¼-çœŸå®å€¼?
**** å›¢é˜Ÿ
**** å¸‚åœº
**** ä¸šç»©
ç”¨æˆ·dauç­‰æ•°æ®

*** å­¦ä¹ ç‡
å´æ©è¾¾æ›´å€¾å‘äºçœ‹æ›²çº¿å›¾, è€Œéä¾èµ–è‡ªåŠ¨æ”¶æ•›æµ‹è¯•ã€‚
ä»£ç å¦‚ä½•è‡ªåŠ¨åˆ†ææ›²çº¿å›¾å‘¢? å’Œè‡ªåŠ¨æ”¶æ•›æµ‹è¯•æœ¬è´¨ä¸æ˜¯ä¸€æ ·çš„å—?

learning rate alphaä»£è¡¨çš„æ˜¯æ­¥é•¿?
ä¸ºä»€ä¹ˆé€‰æ‹©æ›´å°çš„é˜¿å°”æ³•(alpha)?
alphaä¹Ÿä¸èƒ½å¤ªå°ï¼Œå¦åˆ™å¯èƒ½æ”¶æ•›å¾—å¾ˆæ…¢ã€‚

å´å–œæ¬¢ä»[...,0.001,0.003,0.01,0.03,0.1,0.3,1,...]è¿™æ ·çš„èŒƒå›´æ‰¾alphaã€‚
3å€å–ç‚¹ã€‚è¿™æ ·æ‰¾åˆ°ä¸€ä¸ªå¤ªå°çš„å€¼ï¼Œå†æ‰¾ä¸ªå¤ªå¤§çš„å€¼ã€‚å¦‚ä½•é€‰æ‹©æœ€å¤§çš„å¯ç”¨å€¼ã€‚

æœ‰æ—¶é€šè¿‡å®šä¹‰æ–°çš„ç‰¹æ€§,æ¯”å¦‚ç”¨é¢ç§¯è€Œä¸æ˜¯é•¿å’Œå®½, å¯ä»¥å¾—åˆ°æ›´å¥½çš„æ¨¡å‹
** æ­£è§„æ–¹ç¨‹
x'xä¸å¯é€†çš„æƒ…å†µå¾ˆå°‘å‡ºç°ï¼Œè€Œä¸”å³ä½¿å‡ºç°, octaveä¸­, pinv(x'x)*x'*yå¯ä»¥å¤„ç†x'xä¸å¯é€†çš„æƒ…å†µ
numpyä¹Ÿå¯å¤„ç†è¿™ä¸ªæƒ…å†µ

*** çŸ©é˜µä¸å¯é€†çš„ä¸¤ç§æƒ…å†µ
1) Xéåˆ—æ»¡ç§©?
è¡Œåˆ—å¼æŸä¸¤è¡Œorä¸¤åˆ—æˆæ¯”ä¾‹ï¼Œè¡Œåˆ—å¼ä¸º0ï¼Œå³çŸ©é˜µä¸å¯é€†ã€‚?
å…·ä½“è®²å°±æ˜¯ä¸¤ä¸ªç‰¹å¾çº¿æ€§ç›¸å…³ã€‚å¦‚x1=size in feet^2, x2=size in m^2
2) ç‰¹å¾å¤ªå¤šï¼Œä»¥è‡´æ ·æœ¬æ•°m<=ç‰¹å¾æ•°n
åŠæ³•:åˆ ä¸€äº›ç‰¹å¾; æˆ–æ­£è§„åŒ–

? ç›¸å½“äºæ‰¾æœ€å¤§çº¿æ€§æ— å…³ç»„
? çº¿æ€§å˜æ¢
* æœºå™¨å­¦ä¹ å®æˆ˜

ä¸€äº›å±æ€§å…·æœ‰é•¿å°¾åˆ†å¸ƒï¼Œå› æ­¤ä½ å¯èƒ½è¦å°†å…¶è¿›è¡Œè½¬æ¢ï¼ˆä¾‹å¦‚ï¼Œè®¡ç®—å…¶logå¯¹æ•°ï¼‰

ç”¨ä¸­ä½æ•°å¡«å……è®­ç»ƒé›†çš„ç¼ºå¤±å€¼æ—¶ï¼Œä¹Ÿè¦ç”¨è¯¥ä¸­ä½æ•°å¡«å……æµ‹è¯•é›†ä¸­çš„ç¼ºå¤±å€¼ï¼Œä¹Ÿå¯ä»¥ç”¨æ¥å®æ—¶æ›¿æ¢æ–°æ•°æ®ä¸­çš„ç¼ºå¤±å€¼ã€‚

å¯¹å¤æ‚é—®é¢˜(å¦‚è‡ªç„¶è¯­è¨€æ­§ä¹‰æ¶ˆé™¤)è€Œè¨€ï¼Œæ•°æ®æ¯”ç®—æ³•æ›´é‡è¦ã€‚

p53, ä¸ºä»€ä¹ˆnp.random.seed(42)å¯ä»¥è®©æ¯æ¬¡æ‰§è¡Œçš„æ•°æ®æ˜¯ä¸€æ ·çš„?
** ç¬¬äºŒç« 
*** æ•°æ®å‡†å¤‡
idæ¶‰åŠåˆ°æµ‹è¯•é›†çš„ç¨³å®šæ€§ï¼Œå³æ–°å®ä¾‹åŠ å…¥æ—¶ï¼Œæ–°å®ä¾‹é›†çš„20%åŠ å…¥æµ‹è¯•é›†ï¼Œæµ‹è¯•é›†åŸæ¥çš„æ•°æ®ä¸å˜ã€‚//p53
ç”¨æœ€ç¨³å®šçš„ç‰¹å¾åˆ›å»ºidã€‚ç»çº¬åº¦æ˜¯ä¸€ä¸ªä¾‹å­ã€‚

å»ºç«‹è‡ªå·±çš„è½¬æ¢å‡½æ•°åº“

ä¸­ä½æ•°å’Œå¹³å‡æ•°ä¸åŒ

dropå‡½æ•°é»˜è®¤åˆ é™¤è¡Œï¼Œåˆ—éœ€è¦åŠ axis = 1

ä¼°ç®—å™¨çš„å…³é”®è¯æ˜¯fit:æ¯”å¦‚å°†ç¼ºå¤±å€¼ç”¨ä¸­ä½æ•°ä¼°ç®—å¡«å……ã€‚
è½¬æ¢å™¨çš„å…³é”®è¯æ˜¯transform: è½¬æ¢å™¨å±äºä¼°ç®—å™¨çš„ä¸€ç§ã€‚èƒ½è½¬æ¢æ•°æ®é›†çš„ä¼°ç®—å™¨ã€‚
é¢„æµ‹å™¨ä¹Ÿæ˜¯ä¼°ç®—å™¨çš„ä¸€ç§:èƒ½åŸºäºç»™å®šçš„æ•°æ®é›†è¿›è¡Œé¢„æµ‹ã€‚

å¤„ç†æ–‡æœ¬:
å¤§éƒ¨åˆ†æœºå™¨å­¦ä¹ ç®—æ³•æ›´æ˜“äºå’Œæ•°å­—æ‰“äº¤é“ï¼Œéœ€è¦å°†æ–‡æœ¬æ ‡ç­¾è½¬ä¸ºæ•°å­—ã€‚
ç®—æ³•ä¼šä»¥ä¸º2ä¸ªç›¸è¿‘æ•°å­—æ›´ä¸ºç›¸ä¼¼ä¸€äº›ã€‚å¦‚æœçœŸå®æƒ…å†µå¹¶éå¦‚æ­¤ï¼Œå¯ä»¥ç”¨OneHotEncoderå°†æ•´æ•°åˆ†ç±»å€¼è½¬ä¸ºç‹¬çƒ­å‘é‡ã€‚
ç‹¬çƒ­å‘é‡æ— ç›¸è¿‘æ•°å­—ï¼Œå‡æ˜¯æŸä¸€ä¸ªå…ƒç´ æ˜¯1å…¶ä½™ä¸º0çš„å‘é‡ï¼Œé¿å…äº†ç›¸ä¼¼åº¦è¯¯åˆ¤ã€‚

æ ‡å‡†åŒ–:å¹³å‡å€¼ä½œä¸ºå‚ç…§æ ‡å‡†ï¼Œæ•…æ›°æ ‡å‡†åŒ–ã€‚
*** é€‰æ‹©å’Œè®­ç»ƒæ¨¡å‹
fitä¹‹åå°±å¯ä»¥å¾—åˆ°æ¨¡å‹äº†ï¼Œå°±å¯ä»¥é¢„æµ‹predictäº†

MSE: mean squared error
RMSE: root mean squared error

*** äº¤å‰éªŒè¯
äº§ç”ŸèƒŒæ™¯: å†³ç­–æ ‘åœ¨æ•´ä¸ªè®­ç»ƒé›†è¿‡æ‹Ÿåˆäº†ï¼Œåˆä¸èƒ½åŠ¨æµ‹è¯•é›†ï¼Œæœ‰ä¸ªåŠæ³•å°±æ˜¯ç”¨éƒ¨åˆ†è®­ç»ƒé›†è®­ç»ƒï¼Œå‰©ä½™è®­ç»ƒé›†éªŒè¯ã€‚äºæ˜¯æœ‰äº†äº¤å‰éªŒè¯ã€‚


housing.iloc[:5]

*** ç½‘æ ¼æœç´¢
?ä»€ä¹ˆæ˜¯ç½‘æ ¼æœç´¢
ç½‘æ ¼æœç´¢æ˜¯ä¸€ç§é€šè¿‡éå†ç»™å®šçš„å‚æ•°ç»„åˆ(æ¯”å¦‚å†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦)æ¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°çš„æ–¹æ³•ã€‚
å‚æ•°ç»„åˆå½¢æˆçš„ç©ºé—´ï¼Œç±»ä¼¼ç½‘æ ¼ï¼Œæ•…åç½‘æ ¼æœç´¢ã€‚

ç»™å‡ºä¸€ç³»åˆ—çš„æœ€å¤§æ·±åº¦çš„å€¼ï¼Œæ¯”å¦‚ {'max_depth': [1,2,3,4,5]}ï¼Œå¸Œæœ›é€‰æ‹©æœ€ä¼˜æœ€å¤§æ·±åº¦ã€‚
å¦‚ä½•è¯„ä¼°å“ªä¸ªæœ€å¤§æ·±åº¦çš„æ¨¡å‹æ˜¯æœ€ä¼˜çš„å‘¢? å…¶ä¸­ä¸€ä¸ªç»å…¸çš„æ–¹æ³•æ˜¯KæŠ˜äº¤å‰éªŒè¯ã€‚

? ä½•æ—¶ç”¨gridsearch
éœ€è¦ä¼˜åŒ–æ¨¡å‹æ—¶ã€‚
å¦‚ç›´æ¥ç”¨å†³ç­–æ ‘å¾—åˆ°çš„åˆ†æ•°å¤§çº¦æ˜¯92%ï¼Œç»è¿‡ç½‘æ ¼æœç´¢ä¼˜åŒ–ä»¥åï¼Œå¯ä»¥åœ¨æµ‹è¯•é›†å¾—åˆ°95.6%çš„å‡†ç¡®ç‡ã€‚

? ç½‘æ ¼æœç´¢ä¼šè‡ªåŠ¨æŸ¥æ‰¾æ˜¯å¦æ·»åŠ æˆ‘ä»¬ä¸ç¡®å®šçš„ç‰¹å¾
** ç¬¬å››ç« 

np.c_æ˜¯concateä¸¤ä¸ªçŸ©é˜µ

Octaveæœ‰pinvè®¡ç®—ä¼ªé€†çŸ©é˜µ, numpyä¹Ÿæœ‰numpy.linalg.pinv()

?? å¦‚ä½•ä»ä¸Šå±±æ”¹ä¸ºä¸‹å±±
Once you have the gradient vector, which points uphill, just go in the opposite direcâ€ tion to go downhill.


? To find a good learning rate, you can use grid search(chp2)
|          |                  | å¤æ‚åº¦                               | ç¼ºç‚¹                                                  |
| æ¢¯åº¦ä¸‹é™ | é€æ­¥è¿­ä»£         |                                      | éœ€è¦æ‰€æœ‰ç‰¹å¾å€¼çš„æ¯”ä¾‹å·®ä¸å¤š,å¦åˆ™æ”¶æ•›æ…¢(ç«–é•¿çš„ç­‰é«˜çº¿å›¾) |
| æ­£è§„æ–¹ç¨‹ | ä¸€æ¬¡æ€§ç›´æ¥å‡ºç»“æœ | O(n^2.4)~O(n^3),nç‰¹å¾æ•°;O(m),mæ ·æœ¬æ•° | ç‰¹å¾æ•°å¤§(æ¯”å¦‚10ä¸‡çº§)æ—¶è®¡ç®—ç¼“æ…¢                        |

O(m)å¦‚ä½•å¾—å‡ºçš„?

*** æ‰¹é‡æ¢¯åº¦ä¸‹é™
? æ‰¹é‡æ˜¯é’ˆå¯¹æ ·æœ¬è¿˜æ˜¯ç‰¹å¾
//é’ˆå¯¹æ ·æœ¬

æ”¶æ•›ç‡: æˆæœ¬å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œä¸”æ–œç‡æ²¡æœ‰æ˜æ˜¾å˜åŒ–æ—¶ï¼Œä¸€ä¸ªå›ºå®šçš„å­¦ä¹ ç‡æœ‰ä¸€ä¸ªæ”¶æ•›ç‡ã€‚ ä¸ºO(1/è¿­ä»£æ¬¡æ•°)

å­¦ä¹ ç‡ä¸ºä½•å‘½åä¸ºå­¦ä¹ ç‡?
æ˜¯å¦å¯ä»¥ç†è§£ä¸ºå­¦ä¹ çš„æ•ˆç‡: è¿‡å°åˆ™æ”¶æ•›å¤ªæ…¢ï¼Œè¿‡å¤§åˆ™å¯èƒ½æ— æ³•æ”¶æ•›ã€‚
*** éšæœºæ¢¯åº¦ä¸‹é™(SGD)
æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸€èˆ¬æ˜¯ç¨³æ­¥ä¸‹é™ï¼ŒSGDæ˜¯ä¸€ä¸Šä¸€ä¸‹æ¥å›è·³åœ°å¾€ä¸‹é™ã€‚
SGDåˆ°äº†æœ€ä¼˜è§£é™„è¿‘ä¼šç»§ç»­è·³è·ƒï¼Œå¯¼è‡´æœ€åå¾—åˆ°çš„æ˜¯è¿‘ä¼¼æœ€ä¼˜è§£ã€‚

?? ä¸ºä»€ä¹ˆè¯´:
There is almost no difference after training: all these algorithms end up with very similar models and make predictions in exactly the same way.

æ­£è§„æ–¹ç¨‹åªèƒ½ç”¨åœ¨çº¿æ€§å›å½’

å­¦ä¹ æ›²çº¿:xè½´æ˜¯è®­ç»ƒæ ·æœ¬æ•°ï¼Œyè½´æ˜¯rmseã€‚è®­ç»ƒçš„è¿‡ç¨‹æ˜¯å­¦ä¹ è¿‡ç¨‹ï¼Œæ•…å«å­¦ä¹ æ›²çº¿ã€‚

?ä½•æ—¶ç”¨äº¤å‰éªŒè¯
//éœ€è¦ä¼°è®¡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ—¶
If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it perâ€ forms poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.

If your model is underfitting the training data, adding more trainâ€ ing examples will not help. You need to use a more complex model or come up with better features.
æ‹Ÿåˆä¸è¶³è¦ä¹ˆæ¨¡å‹ä¸è¡Œï¼Œè¦ä¹ˆç‰¹å¾ä¸è¡Œ

One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error.
è¿‡æ‹Ÿåˆæ—¶ï¼Œç”¨æ›´å¤šè®­ç»ƒæ•°æ®ç›´åˆ°éªŒè¯è¯¯å·®æ¥è¿‘è®­ç»ƒè¯¯å·®
* d2l
** installation
conda activate d2l
Could not find conda environment: d2l
åŸå› :æ²¡åˆ›å»ºç¯å¢ƒ
è§£å†³:conda create --name d2l -y

(base) win:dev win$ conda activate d2l
(d2l) win:dev win$

æ›´æ–°conda
conda update -n base -c defaults conda

pip install torch==1.5.0 torchvision
ERROR: torchvision 0.6.1 has requirement torch==1.5.1, but you'll have torch 1.5.0 which is incompatible.
pip install torch==1.5.1 torchvision


Please always execute conda activate d2l to activate the runtime environment before running the code of the book or updating MXNet or the d2l package. To exit the environment, run conda deactivate.

ModuleNotFoundError: No module named 'd2l'
è§£å†³:
pip install -U d2l -f https://d2l.ai/whl.html


(base) striker:~ striker$ conda activate d2l
(d2l) striker:~ striker$ pip install -U d2l -f https://d2l.ai/whl.html
baseå’Œd2lè¿™2ä¸ªä¸åŒç¯å¢ƒçš„packagesçš„ä¸å…±äº«çš„ã€‚
baseå®‰è£…è¿‡numpy, åˆ°äº†d2léœ€è¦é‡è£…ã€‚


** instroduction
Sequence Learning: ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚è€ƒè™‘è¿‡å»å’Œå°†æ¥çš„æ ·æœ¬
å“ªäº›é—®é¢˜ä¸éœ€è¦Sequence Learning? 
problems where we have some fixed number of inputs and produce a fixed number of outputs.
åé¢çš„è¾“å…¥å’Œä¹‹å‰çš„è¾“å…¥æ²¡æœ‰å…³ç³»çš„åœºæ™¯

ä¸ºä½•å«æ·±åº¦å­¦ä¹ :
Deep models are deep in precisely the sense that they learn many layers of computation.

distribution shift
ç©¿è¶Š?

reinforcement learning and adversarial learning
*** å¼ºåŒ–å­¦ä¹ 
actuator
The behavior of an RL agent is governed by a policy. In short, a policy is just a function that maps from observations (of the environment) to actions. The goal of reinforcement learning is to produce a good policy.

we can cast any supervised learning problem as an RL problem.
æœ‰æ—¶éœ€è¦æ”¾å¼ƒçŸ­æœŸåˆ©ç›Š:
potentially giving up some short-run reward in exchange for knowledge.

é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹
When the environment is fully observed, we call the RL problem a Markov Decision Process (MDP). 
ä¸Šä¸‹æ–‡èµŒåšæœºã€‚åå­—æœ‰ç‚¹æƒ…å†µï¼Œå’Œä¹‹å‰çš„actionæ— å…³ï¼Œä¸ºä½•è¿˜è¯´æ˜¯ä¸Šä¸‹æ–‡?
When the state does not depend on the previous actions, we call the problem a contextual bandit problem. 
When there is no state, just a set of available actions with initially unknown rewards, this problem is the classic multi-armed bandit problem.


the autograd package automatically computes differentiation for us

** preliminaries
torch.randn(3, 4)
Each of its elements is randomly sampled from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1.

ä¸ºä»€ä¹ˆæœ‰çš„å…ƒç´ å¤§äº1

a = torch.exp(torch.ones(2, 2))  # å¾—åˆ°2*2çš„å…¨æ˜¯eçš„Tensor
print(a)
print(torch.log(a))  # å–è‡ªç„¶å¯¹æ•°

>>> x
tensor([1., 2., 4., 8.])
>>> torch.exp(x)
tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])

e^1, e^2, e^4, e^8


broadcast: for matrix a it replicates the columns and for matrix b it replicates the rows before adding up both elementwise.
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
aåŠ ä¸€åˆ—å…ˆå˜ä¸º
[0],[0]
[1],[1]
[2],[2]
båŠ 2è¡Œå˜ä¸º
0,1
0,1
0,1
ç„¶å2ä¸ªçŸ©é˜µå†ç›¸åŠ 

If the value of x is not reused in subsequent computations, we can also use x[:] = x + y or x += y to reduce the memory overhead of the operation.

>>> a
tensor([[[ 0.,  1.,  2.],
         [ 3.,  4.,  5.],
         [ 6.,  7.,  8.]],

        [[ 9., 10., 11.],
         [12., 13., 14.],
         [15., 16., 17.]],

        [[18., 19., 20.],
         [21., 22., 23.],
         [24., 25., 26.]]])
>>> b = torch.tensor([1,1,1,1]);
>>> b
tensor([1, 1, 1, 1])
>>> c =  a +b
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 2
>>>
>>> b = torch.tensor([[[[1]]]]); #[[[[4ä¸ªæ‹¬å·
>>> b
tensor([[[[1]]]])
>>> c =  a +b    #??è¿™é‡Œä¸ºä»€ä¹ˆå¯ä»¥åŠ ?
>>> c
tensor([[[[ 1.,  2.,  3.],
          [ 4.,  5.,  6.],
          [ 7.,  8.,  9.]],

         [[10., 11., 12.],
          [13., 14., 15.],
          [16., 17., 18.]],

         [[19., 20., 21.],
          [22., 23., 24.],
          [25., 26., 27.]]]])
>>> b2 = torch.tensor([[[1]]]);
>>> b2
tensor([[[1]]])
>>> a+b2
tensor([[[ 1.,  2.,  3.],
         [ 4.,  5.,  6.],
         [ 7.,  8.,  9.]],

        [[10., 11., 12.],
         [13., 14., 15.],
         [16., 17., 18.]],

        [[19., 20., 21.],
         [22., 23., 24.],
         [25., 26., 27.]]])
>>> a.size() torch.Size([3, 3, 3])
>>> b.size()
torch.Size([1, 1, 1, 1])
>>> b2.size()
torch.Size([1, 1, 1])

ä¸ºä»€ä¹ˆpythonå‘½ä»¤è¡Œå¯ä»¥import torch, è€Œåœ¨notebooké‡ŒæŠ¥ModuleNotFoundError: No module named 'torch'

The shape is a tuple that lists the length (dimensionality) along each axis of the tensor. 
//shapeæè¿°çš„æ˜¯å¼ é‡åœ¨ç©ºé—´ä¸­çš„å¤–å½¢

A_sum_axis0 = A.sum(axis=0)
æŒ‰è¡Œå–å…ƒç´ ï¼Œåˆ—ä¿ç•™ï¼Œæ¶ˆé™¤è¡Œ
//è¢«sumçš„axisè¢«reduceäº†

A_sum_axis1 = A.sum(axis=1)
è¡Œä¿ç•™ï¼Œæ¶ˆé™¤åˆ—

Frobenius norm of a matrix ||X||_Fæ˜¯general norm ||X||_pçš„ä¸€ä¸ªä¾‹å­

differential calculus å¾®åˆ†
integral calculus ç§¯åˆ†

åå¾®åˆ†æ˜¯æ ‡é‡ï¼Œæ¢¯åº¦å‘é‡æ˜¯å‘é‡

2.4.3æ¢¯åº¦ç›¸å…³å…¬å¼ï¼Œå¦‚æœæœ‰å…·ä½“çš„ä¾‹å­è¾…åŠ©è§£æï¼Œä¼šæ›´è®©äººæ›´å®¹æ˜“ç†è§£ã€‚


*** 2.4.6 ex
2.  [6x1, 5e^x2]^T

    
*** 2.5 è‡ªåŠ¨å¾®åˆ†
? ä¸ºä»€ä¹ˆç”¨backpropagate 

Note the requires_grad=True argument when creating x, it tells the framework we need allocate gradient space for x in the future.
é¿å…æ¯æ¬¡åˆ†é…å†…å­˜å ç”¨å¤ªå¤šå†…å­˜ã€‚

The gradient of the function  y=2ğ±^T ğ±  with respect to  ğ±  should be  4ğ± . 
x^T * x å¯ä»¥è®¤ä¸ºæ˜¯x^2, æ•…yå¯¹xæ±‚å¯¼æ˜¯4x

xæ˜¯ tensor([0., 1., 2., 3.], requires_grad=True))
? ä¸ºä»€ä¹ˆx.gradæ˜¯ tensor([0., 1., 4., 9.])

?Computing the Gradient of Python Control Flow
?piecewise linear

?We then record the computation of our target value, execute its backward function, and access the resulting gradient via our variable's grad attribute.

*** æ¦‚ç‡
å¤§æ•°å®šå¾‹å¦‚æ­¤ç®€å•:
The law of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. 
** ch3 linear neural networks

classic statistical learning techniques such as linear and softmax regression can be cast as linear neural networks. 

Not every prediction problem is a classic regression problem. 

maximum likelihood estimators æœ€å¤§ä¼¼ç„¶ä¼°è®¡
While, maximizing the product of many exponential functions, might look difficult, we can simplify things significantly, without changing the objective, by maximizing the log of the likelihood instead. 

Since for linear regression, every input is connected to every output
è¿™å°±å«ully-connected layer or dense layer
å…¨è¿æ¥å±‚ã€ç¨ å¯†å±‚

æ ‡å‡†åˆ†å¸ƒã€æ­£æ€åˆ†å¸ƒçš„åŒºåˆ«?

features[:, 1] æ‰€æœ‰è¡Œ, ç¬¬äºŒåˆ—
features[:, 0] æ‰€æœ‰è¡Œ, ç¬¬ä¸€åˆ—

* tfä½¿ç”¨
AttributeError: module 'tensorflow' has no attribute 'Session'
tf.compat.v1.Session()

2.Xç”¨äº†eager_execution
* æ•°æ®æŒ–æ˜ç®—æ³•ç«èµ›
KæŠ˜äº¤å‰éªŒè¯: D1,D2,...,D10.
D1,...,D9å»éªŒè¯D10
é™¤å»D9çš„å…¶ä»–9ä¸ªå»éªŒè¯D9
...
å»é™¤D1çš„å…¶ä»–9ä¸ªå»éªŒè¯D1


** æ•°æ®ç©¿è¶Š
*** æ—¶é—´ç©¿è¶Š
è¯„ä¼°ç©¿è¶Šï¼ŒæŒ‡çš„å°±æ˜¯ç”±äºæ ·æœ¬åˆ’åˆ†ä¸å½“ï¼Œå¯¼è‡´æµ‹è¯•é›†ä¸­çš„ä¿¡æ¯â€œç©¿è¶Šâ€åˆ°äº†è®­ç»ƒé›†ä¸­ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¼šæ›´åçˆ±è¿‡æ‹Ÿåˆçš„æ¨¡å‹ï¼Œä»è€Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¤Ÿå‡†ç¡®ã€‚
å‡å¦‚æ ·æœ¬æ•°æ®å«7ã€8æœˆä»½æ•°æ®ï¼Œ8:2æŒ‰éšæœºæ–¹å¼é€‰æµ‹è¯•æ•°æ®æ—¶ï¼Œè®­ç»ƒæ•°æ®7ã€8æœˆä»½çš„éƒ½æœ‰ï¼ŒéªŒè¯æ•°æ®ä¹Ÿæœ‰7ã€8æœˆä»½çš„, å³éªŒè¯æ•°æ®ä¸èƒ½ä¿è¯æ¯”è®­ç»ƒæ•°æ®æ–°ã€‚
è€Œæ¨¡å‹ä½¿ç”¨æ—¶ï¼Œå¾€å¾€å¤„ç†çš„æ•°æ®éƒ½æ˜¯æ–°æ•°æ®ã€‚
è¿™å°±å¯¼è‡´æ¨¡å‹çš„è®­ç»ƒåœºæ™¯å’Œå®é™…é¢„æµ‹æ—¶çš„åœºæ™¯ä¸åŒã€‚
å‚è€ƒhttps://blog.csdn.net/phrmgb/article/details/79997057

å¦‚æœæ—¶é—´å±æ€§å¯¹æ•°æ®æ— ç”šå½±å“ï¼Œè¿™ä¸ªæ—¶é—´ç©¿è¶Šçš„é—®é¢˜å°±ä¸å¤§ã€‚

æµ‹è¯•é›†å’ŒéªŒè¯é›†çš„åŒºåˆ«?


ç¼ºå¤±å€¼å¯èƒ½æœ‰ä¸šåŠ¡å«ä¹‰ 
